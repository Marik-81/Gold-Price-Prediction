{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# The final, corrected URL template starting from 2020-01-01\n",
    "BASE_API_URL_TEMPLATE = (\n",
    "    \"The Website You Want to Scape it\"\n",
    "    \"sort=date:desc&fields[]=rates&fields[]=date&filters[currency][$eq]=USD&\"\n",
    "    \"filters[date][$gte]=2020-01-01T00:00:00.000Z&\" \n",
    "    \"filters[date][$lte]=2025-10-21T20:59:59.999Z&\"\n",
    "    \"pagination[page]={page_num}&pagination[pageSize]=100\" \n",
    ")\n",
    "\n",
    "# CRITICAL: PASTE YOUR FULL, WORKING HEADERS HERE (including the Authorization token)\n",
    "HEADERS = {\n",
    "    \"Authorization\": \"Bearer YOUR_ACTUAL_HIDDEN_API_KEY_HERE\", \n",
    "    \"User-Agent\": \"Your USER AGENT\",\n",
    "    \"Referer\": \"The Website You Want to Scape it (Check the Robots.txt & POS )\", \n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Accept-Encoding\": \"Go Through \",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Origin\": \"The Website You Want to Scape it\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Sec-Fetch-Site\": \"same-site\",\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_gold_data(page_num):\n",
    "    \"\"\"Fetches data for a specific page number.\"\"\"\n",
    "    url = BASE_API_URL_TEMPLATE.format(page_num=page_num)\n",
    "    \n",
    "    try:\n",
    "        print(f\"‚û°Ô∏è Fetching page {page_num}...\")\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        response.raise_for_status() \n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"‚ùå HTTP Error on page {page_num}: {err}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred on page {page_num}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # ETHICAL DELAY\n",
    "        print(\"‚è∏Ô∏è Pausing for 7 seconds...\")\n",
    "        time.sleep(7)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    all_records = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        data = fetch_gold_data(page)\n",
    "        \n",
    "        if data and data.get('data'):\n",
    "            records_on_page = data['data']\n",
    "            \n",
    "            # Stop condition: if the server returns an empty list\n",
    "            if not records_on_page:\n",
    "                print(f\"üèÅ Page {page} returned no data. End of historical records.\")\n",
    "                break\n",
    "                \n",
    "            print(f\"‚úÖ Success! Retrieved {len(records_on_page)} records on page {page}.\")\n",
    "            all_records.extend(records_on_page)\n",
    "            page += 1\n",
    "            \n",
    "        else:\n",
    "            print(f\"üèÅ Request failed or page {page} returned no data. End of process.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"\\n‚ú® Scraping Complete! Total records retrieved: {len(all_records)}\")\n",
    "    \n",
    "    # --- FINAL DATA PROCESSING AND SAVE ---\n",
    "    if all_records:\n",
    "        df_final = pd.DataFrame(all_records)\n",
    "        \n",
    "        # 1. Date cleaning and validation\n",
    "        df_final['date'] = pd.to_datetime(df_final['date'], errors='coerce')\n",
    "        df_final_cleaned = df_final.dropna(subset=['date']).copy()\n",
    "        \n",
    "        # 2. Extract 24k gold price (USD)\n",
    "        # We need to handle the 'rates' column which is a dictionary\n",
    "        df_final_cleaned['price'] = df_final_cleaned['rates'].apply(lambda x: x.get('24k') if isinstance(x, dict) else None)\n",
    "        df_final_cleaned = df_final_cleaned.dropna(subset=['price'])\n",
    "        \n",
    "        # 3. Save final cleaned data\n",
    "        df_output = df_final_cleaned[['date', 'price']].rename(columns={'price': 'price_24k_usd'})\n",
    "        \n",
    "        output_filename = 'gold_sa_historical_data_2020_onwards.csv'\n",
    "        df_output.to_csv(output_filename, index=False)\n",
    "        print(f\"Data saved to {output_filename}. Total cleaned records: {len(df_output)}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No records were successfully retrieved to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
